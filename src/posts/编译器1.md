---
title: 自制编译器 一
description: 词法分析
---

本系列考虑使用Rust编写一个功能完整的编程语言。无论是什么编程语言，源代码始终是一连串的字符，即一个编译器的输入往往是一连串的字符。那么编译器的第一个部分、最前端的前端，就是词法分析器了。

## 创建项目
此文章是系列的第一篇文章，自然要从创建项目开始。在命令行输入：
```shell
cargo new compiler-rust
```
进入项目目录后修改`Cargo.toml`
```toml
[package]
name = "compiler-rust"
version = "0.1.0"
edition = "2021"

[[bin]]
name = "compiler"
path = "src/compiler_cli.rs
```
将程序的入口设置为`src/compiler_cli.rs`。
在`src`目录下建立编译器的几个模块，项目目录如下：
├── Cargo.lock
├── Cargo.toml
└── src
    ├── compiler_cli.rs
    └── compiler_core
        ├── ast.rs
        ├── lexer.rs
        ├── mod.rs
        ├── parser.rs
        └── token.rs
在`src/compiler_core/mod.rs`中将其他模块暴露出去
```rust
pub mod ast;
pub mod token;
pub mod parser;
pub mod lexer;
``` 

## 词法单元
词法分析器的任务是将一系列的字符串转化成方便后续处理的词法单元`Token`。词法单元是一个编程语言中不可分割的最小处理单元，流行的编程语言中常见的词法单元有：关键字、运算符、字面量、变量。词法分析器需要根据这些词法单元的“特征”找到它们，如果有词法单元要存储值，则需要把值也保存下来。

根据上述对此法单元的分析，我们将这个概念抽象出来，表达成Rust中的enum。选择enum是有原因的，词法单元的种类很多，但每个种类存储的信息有限：要么不存储值，要么只存储一个值，而Rust中的enum正好符合这个需求。

于是，我们在`token.rs`中定义Token如下：
```rust
#[derive(Debug, PartialEq)]
#[allow(dead_code, non_camel_case_types)]
pub enum Token {
    ILLEGAL,
    EOF,
    IDENT(String),
    NUMBER(f64),
    STRING,
    ASSIGN,
    PLUS,
    MINUS,
    ASTERISK,
    SLASH,
    EXCLAMATION,
    GT
    ...
```
目前，有值的Token只有`IDENT`和`NUMBER`。

==更新==
后来我发现enum并不是十分合适，由于NUMBER存储f64，这会导致Token难以实现Clone trait。然后我还希望Token能够记录它所在的代码行数，这样在出错的时候可以给出行数信息，有助于调试。

因此，我把Token改成了TokenType，TokenType是一个没有值的enum，然后新增了一个struct Token，它存储lexeme和行号line：
```rust
#[derive(Debug, PartialEq, Clone)]
pub struct Token {
    pub token_type: TokenType,
    pub lexeme: String,
    pub line: i32
}
```


## 词法分析器
词法分析器的成员变量无非是一个字符串，但Rust中有很多字符串数据结构，我们该如何选择呢？我们先想想我们的需求，我们需要词法分析器能很方便的对字符串遍历，我们还需要词法分析器能够预读，即能够读到下一个字符但不移动当前字符位置。
在Rust中`Chars`就是遍历字符串切片的迭代器，而Rust中的`Peekable`结构体正好满足我们的第二个需求。于是我们定义词法分析器结构体如下：
```rust
pub struct Lexer<'a> {
    expr: Peekable<Chars<'a>>

    pub fn new(new_expr: &'a str) -> Self {
        Lexer { expr: new_expr.chars().peekable() }
    }
}
```
我们在`new`函数中将字符串切片转化成了`Peekable<Chars>`结构，并确保结构的生命周期。

接下来，我们希望这个词法分析器能够实现`Iterator`接口，这样就可以通过`next`方法获取下一个词法单元了。
```rust
impl<'a> Iterator for Lexer<'a> {
    type Item = Token;
    fn next(&mut self) -> Option<Self::Item> {
        ...
    }
} 
```

### 跳过空格
在我们的编程语言中，空格只作为排版美观的工具，而不像Python那样对空格有特殊要求。因此空格对于我们的编译器分析来说是多余的，我们需要在词法分析的时候就把它们去除。
这段逻辑实现在一个无限循环中，只要当前位置是“空格”，就不断消费当前字符，直到遇到非空格或EOF时结束
```rust
fn skip_whitespace(&mut self) {
    let whitespace = " \t\n\r";
    loop {
        // 一直找空格
        match self.expr.peek() {
            None => return,
            Some(n) => {
                if whitespace.contains(*n) {
                    self.expr.next();
                } else {
                    return;
                }
            }
        }
        
    }
}
```
我们在每次调用next时候，希望能跳过多余的空格，于是`skip_whitespace`是next方法在第一行调用的方法。

### 读取数字
考虑使用一个`match`语句对不同的字符进行分析，于是在next方法中首先读出一个非空格的字符，然后根据这个字符来考虑下一步工作：
```rust
let next_char = self.expr.next();

match next_char {
    Some('0'..='9') => {
        ...
```
在遇到字符0到字符9的时候，说明待处理的是数字，我们接着往后读，遇到其他数字或是`.`就加入缓冲，否则停止返回，代码如下：
```rust
let mut number = next_char?.to_string();

while let Some(next_char) = self.expr.peek() {
    if next_char.is_numeric() || next_char == &'.' {
        number.push(self.expr.next()?);
    } else {
        break;
    }
}
// 返回数字
Some(Token::NUMBER(number.parse::<f64>().unwrap()))
```

### 预读
有些词法单元由两个或更多字符组成，比如`==`，因此我们不能仅仅凭当前字符的信息确认词法单元，我们需要往后多读一个字符。比如当我们在读到`=`的时候，需要先确认后一个字符是不是也是`=`，如果不是，那么就是ASSIGN词法单元，否则就是EQ词法单元
```rust
Some('=') => {
    // 如果后面跟的是等号，就返回EQ 否则返回赋值号
    if let Some(next_char) = self.expr.peek() {
        if next_char == &'=' {
            // 往后读一位
            self.expr.next();
            return Some(Token::EQ) 
        }
    }
    Some(Token::ASSIGN)
},
```

由于很多token都是由两个字符组成，我们可以把预读的逻辑抽出来，作为一个函数`expected`：

```rust
    fn expected(&mut self, expected: char) -> bool {
        match self.expr.peek() {
            Some(&actual) if actual == expected => {
                self.expr.next();
                true
            },
            _ => false
        }
    }
```
上面关于`==`的arm可以改写为：
```rust
Some('=') => {
    // 如果后面跟的是等号，就返回EQ 否则返回赋值号
    if self.expected('=') {
        Some(Token::new(TokenType::EQ, "==".to_string(), self.line))
    } else {
        Some(Token::new(TokenType::ASSIGN, "=".to_string(), self.line))
    }
},
```

### 注释
我们使用`//`来注释一行代码，即从`//`开始到换行符结束的内容都作为注释，由于它也是由两个字符组成的，所以也需要用到`expected`：
```rust
Some('/') => {
    if self.expected('/') {
        // 如果是注释，就一直读到换行符
        while let Some(&next_char) = self.expr.peek() {
            if next_char != '\n' {
                self.expr.next();
            } else {
                break;
            }
        };
        // 返回下一个token
        self.next()
    } else {
        Some(Token::new(TokenType::SLASH, "/".to_string(), self.line))
    }
},
```



### 识别关键字和标识符
在我们的语言中，所有的关键字和标识符都以字母开头，即不存在以特殊字符开头的标识符（比如$a）,这个和C语言不同。
为了完成上述逻辑，我们在最后一个分支中判断当前字符是否是字母开头，如果不是就之间返回非法词法单元。
```rust
Some(_) => {
    if next_char?.is_alphabetic() {
        let mut identifier = next_char?.to_string();
        while let Some(next_char) = self.expr.peek() {
            if next_char.is_alphabetic() {
                identifier.push(self.expr.next()?);
            } else {
                break;
            }
        }
        // 如果是字符，先看下是不是关键字，如果不是关键字就当做Identifier
        Some(loopkup_ident(&mut identifier))
    } else {
        // 其他情况就返回非法token
        Some(Token::ILLEGAL)
    }
}
```


### 测试
我们基本完成了一个词法分析器需要有的功能，接下来进行测试：
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn whitespace_test() {
        let mut lexer = Lexer::new("   1.1+2");
        let expects = [Token::NUMBER(1.1), Token::PLUS, Token::NUMBER(2.0)];
        for expect in expects {
            assert_eq!(lexer.next().unwrap(), expect);
        }
    }
    
    #[test]
    fn keywords_test() {
        let mut lexer = Lexer::new("true false if else return let fn");
        let expects = [
            Token::TRUE,
            Token::FALSE,
            Token::IF,
            Token::ELSE,
            Token::RETURN,
            Token::LET,
            Token::FUNCTION  
        ];
        for expect in expects {
            assert_eq!(lexer.next().unwrap(), expect);
        }
    }
}    
```

运行`cargo test`发现测试通过！


### 编写程序入口
在`compiler_cli.rs`中添加如下代码：
```rust
fn main() {
    let args: Vec<String> = env::args().collect();
    if args.len() != 2 {
        // 如果没有带参数，暂时报错
        print_help_msg();
        return;
    }
    
    match args[1].as_str() {
        "lexer" | "lex" => lexer_begin(),
        _ => print_help_msg()
    };
}
```
我们通过检测命令行参数来选择执行编译器的哪一个模块，当命令行参数为`lexer`或`lex`的时候，我们就循环执行词法分析器，代码逻辑如下：
```rust
fn lexer_begin() {
    let mut in_buf = "".to_string();
    loop {
        // 进入无线循环，持续解析每个输入
        print!("{PROMPT}");
        // 因为效率问题，rust的stdout默认使用缓存
        // 这会导致stdin完成后才会统一输出(将两次输出合并为一次)
        // 我们需要在每次输入前打印出promt，所以这里需要刷新stdout
        let _ = io::stdout().flush();
        match io::stdin().read_line(&mut in_buf) {
            Ok(_) => {
                let lexer = Lexer::new(&in_buf);
                for tok in lexer {
                    println!("{:?}", tok);
                }
                // 因为read_line会在buf后面append，所以每次执行完后需要clear
                in_buf.clear();
            }
            Err(error) => println!("error: {error}")
        }
    }
}
```
需要注意的是，`io::stdin().read_line()`方法会在缓冲区后面添加内容，而不是每次刷新缓冲区，所以在每次分析完后需要手动刷新缓冲区。
另外，rust的stdout默认使用缓存，它会把多次输出合并在一起，但这不是我们想要的行为，于是在每次输出是我们手动刷新缓冲。

### 下一步
目前的分析器以初步成型，可以考虑进入语法分析阶段了。但一个完整的词法分析器或许还需要以下功能：
- 科学计数表示法： 1e10
- 数字分段： 1_000_000
- 更多运算符 >> += && & ~